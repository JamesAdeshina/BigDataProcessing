{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-03T23:01:15.584203Z",
     "start_time": "2025-01-03T23:01:13.471093Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, when, count, avg, \u001B[38;5;28msum\u001B[39m, \u001B[38;5;28mmax\u001B[39m, \u001B[38;5;28mmin\u001B[39m, stddev, desc, to_date\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorAssembler\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregression\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearRegression\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/MSc/SchoolStuff/Bigdata/coursework/project/BigDataProcessing/.venv/lib/python3.11/site-packages/pyspark/ml/__init__.py:22\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one or more\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# contributor license agreements.  See the NOTICE file distributed with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03mDataFrame-based machine learning APIs to let users quickly assemble and configure practical\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;124;03mmachine learning pipelines.\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     23\u001B[0m     Estimator,\n\u001B[1;32m     24\u001B[0m     Model,\n\u001B[1;32m     25\u001B[0m     Predictor,\n\u001B[1;32m     26\u001B[0m     PredictionModel,\n\u001B[1;32m     27\u001B[0m     Transformer,\n\u001B[1;32m     28\u001B[0m     UnaryTransformer,\n\u001B[1;32m     29\u001B[0m )\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline, PipelineModel\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     32\u001B[0m     classification,\n\u001B[1;32m     33\u001B[0m     clustering,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     44\u001B[0m     param,\n\u001B[1;32m     45\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/MSc/SchoolStuff/Bigdata/coursework/project/BigDataProcessing/.venv/lib/python3.11/site-packages/pyspark/ml/base.py:40\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     24\u001B[0m     Any,\n\u001B[1;32m     25\u001B[0m     Callable,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     36\u001B[0m     TYPE_CHECKING,\n\u001B[1;32m     37\u001B[0m )\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m since\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparam\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m P\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inherit_doc\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparam\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mshared\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     43\u001B[0m     HasInputCol,\n\u001B[1;32m     44\u001B[0m     HasOutputCol,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m     Params,\n\u001B[1;32m     49\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/MSc/SchoolStuff/Bigdata/coursework/project/BigDataProcessing/.venv/lib/python3.11/site-packages/pyspark/ml/param/__init__.py:32\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     21\u001B[0m     Any,\n\u001B[1;32m     22\u001B[0m     Callable,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     29\u001B[0m     TYPE_CHECKING,\n\u001B[1;32m     30\u001B[0m )\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjava_gateway\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JavaObject\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DenseVector, Vector, Matrix\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T23:02:22.979997Z",
     "start_time": "2025-01-03T23:02:22.970845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"BigDataProcessing\").getOrCreate()\n"
   ],
   "id": "bc94c964b923b9f2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T22:25:07.938712Z",
     "start_time": "2025-01-03T22:24:54.175620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "fact_sales = spark.read.csv(\"FactInternetSales.csv\", header=True, inferSchema=True)\n",
    "dim_customer = spark.read.csv(\"DimCustomer.csv\", header=True, inferSchema=True)\n",
    "dim_product = spark.read.csv(\"DimProduct.csv\", header=True, inferSchema=True)\n",
    "dim_date = spark.read.csv(\"DimDate.csv\", header=True, inferSchema=True)\n",
    "dim_sales_territory = spark.read.csv(\"DimSalesTerritory.csv\", header=True, inferSchema=True)"
   ],
   "id": "d2ffc509c30d7fee",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 22:25:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Alice| 34|\n",
      "|      Bob| 45|\n",
      "|Catherine| 29|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cache datasets for performance\n",
    "fact_sales.cache()\n",
    "dim_customer.cache()\n",
    "dim_product.cache()\n",
    "dim_date.cache()\n",
    "dim_sales_territory.cache()"
   ],
   "id": "358ccc93863c95e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Descriptive Statistics\n",
    "fact_sales.describe().show()\n",
    "fact_sales.select([(count(when(col(c).isNull(), c)) / count(\"*\")).alias(c) for c in fact_sales.columns]).show()\n"
   ],
   "id": "e3934057aa18e48e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 3: Data Distributions\n",
    "sales_amount_distribution = fact_sales.groupBy(\"SalesAmount\").count().toPandas()\n",
    "sales_amount_distribution.plot(kind=\"hist\", bins=50, title=\"SalesAmount Distribution\", color=\"blue\")\n",
    "plt.show()"
   ],
   "id": "ed032b80e8c9171a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 4: Correlation Analysis\n",
    "correlation_matrix = fact_sales.select(\"SalesAmount\", \"UnitPrice\", \"DiscountAmount\").toPandas().corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ],
   "id": "5615f9cebd24d838"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 5: Relationship Analysis\n",
    "scatter_data = fact_sales.select(\"SalesAmount\", \"UnitPrice\").toPandas()\n",
    "plt.scatter(scatter_data[\"UnitPrice\"], scatter_data[\"SalesAmount\"], alpha=0.5)\n",
    "plt.title(\"SalesAmount vs UnitPrice\")\n",
    "plt.xlabel(\"UnitPrice\")\n",
    "plt.ylabel(\"SalesAmount\")\n",
    "plt.show()"
   ],
   "id": "62268ca51b9c4067"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 6: Time Series Analysis\n",
    "fact_sales = fact_sales.withColumn(\"OrderDate\", to_date(col(\"OrderDateKey\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "sales_trend = fact_sales.groupBy(\"OrderDate\").sum(\"SalesAmount\").orderBy(\"OrderDate\").toPandas()\n",
    "plt.plot(sales_trend[\"OrderDate\"], sales_trend[\"sum(SalesAmount)\"])\n",
    "plt.title(\"Sales Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.show()"
   ],
   "id": "881a514e309abfc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 7: Geographical Analysis\n",
    "geo_sales = fact_sales.join(dim_sales_territory, \"SalesTerritoryKey\").groupBy(\"SalesTerritoryRegion\").sum(\"SalesAmount\").toPandas()\n",
    "geo_sales.plot(kind=\"bar\", x=\"SalesTerritoryRegion\", y=\"sum(SalesAmount)\", title=\"Sales by Region\", color=\"green\")\n",
    "plt.show()"
   ],
   "id": "9a53b1f1348fa612"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 8: Key Performance Indicators\n",
    "top_products = fact_sales.join(dim_product, \"ProductKey\").groupBy(\"EnglishProductName\").sum(\"SalesAmount\").orderBy(desc(\"sum(SalesAmount)\")).limit(10).toPandas()\n",
    "top_products.plot(kind=\"bar\", x=\"EnglishProductName\", y=\"sum(SalesAmount)\", title=\"Top 10 Products by Sales\")\n",
    "plt.show()"
   ],
   "id": "8937f0d6cf083c96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 9: Outlier Detection\n",
    "sales_data = fact_sales.select(\"SalesAmount\").toPandas()\n",
    "sns.boxplot(x=sales_data[\"SalesAmount\"])\n",
    "plt.title(\"SalesAmount Outliers\")\n",
    "plt.show()"
   ],
   "id": "7211a47a053725c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Forecasting\n",
    "assembler = VectorAssembler(inputCols=[\"OrderDateKey\"], outputCol=\"features\")\n",
    "sales_data = fact_sales.withColumn(\"OrderDateKey\", col(\"OrderDateKey\").cast(\"int\"))\n",
    "sales_vector = assembler.transform(sales_data.select(\"OrderDateKey\", \"SalesAmount\"))\n"
   ],
   "id": "546ca23b247a2be2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
